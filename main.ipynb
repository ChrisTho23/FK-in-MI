{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MATS Application - Christophe Thomassin: <br> Factual Knowledge in Transformer Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I am gonna try to back-up my code with some comments for better understandanding as time allows. Although I want to refer to the Google Docs document for an in-depth analysis: [Link](https://docs.google.com/document/d/13CZcQ818lBNqBGEqn3wxq3iVc7uHldprSr2tgTrBUr8/edit?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Task**: <br>\n",
    "I am interested in understanding how Transformer Language Model store and retrieve knowledge, to be specific factual knowledge. Factual knowledge is essential to all possible information processing tasks as it can be considered the underlying assumptions of pretty much every step of reasoning. So, if LLMs were not able to acquire factual knowledge, they would be not much more than random word generators. I have three reasons why I think applying MI to understand how LLMs acquire and store factual knowledge is a good idea:\n",
    "1. I see facutal knowledge as the easiest form of intelligence, the entry point to cognitive capabilities. Hence, I find it intuitive to start with examining factual knowledge when trying \"mechanistically\" unwined the complexity of LLMs. Once understood, factual knowledge will open a lot of doors to dig deeper into the \"mind\" of Transformers.\n",
    "2. Factual knowledge, or in this case the lack of it, can be considered a large driver of Hallucinations. Understanding how and where factual knowledge is stored could allow us to remediate many Hallucinations.\n",
    "3. Factual knowledge, as the name indicates, is based on commonly-known facts which makes it quite easy to evaluate factual knowledge (one would think). A fact can only be True or False.\n",
    "\n",
    "One of the main drawbacks is that, as one could imagine, we cannot expect factual knowledge to be \"universal\". En contraire, because of its factual nature it is most definitely highly dependent of the training data. A model cannot reason that Paris is the capital of France, without being told that it is during pre-training. Hence, we might see that some models have developed specific factual knowledge while others have not. Yet, my hope is that the mechanisims allowing to store and retrieve factual knowledge are somewhat universal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allows to reload packages when reimported\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import transformer_lens.utils as utils\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformer_lens import ActivationCache, FactoredMatrix, HookedTransformer\n",
    "\n",
    "from src.config import DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/chrisoutho_gmail_com/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "login(token=os.getenv(\"HF_TOKEN\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)\n",
    "\n",
    "device: torch.device = utils.get_device()\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  question  answer\n",
      "0        Is Paris the capital of France?\\n    True\n",
      "1        Is Paris the capital of France?\\n    True\n",
      "2  Is the Eiffel Tower located in Paris?\\n    True\n",
      "3   Is Paris known as the City of Light?\\n    True\n",
      "4          Is the Louvre Museum in Rome?\\n   False\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "df = pd.read_csv(DATA / \"fk_samples.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'True': ['Yes', 'yes', 'Sure', 'Correct', 'Certainly', 'Absolutely', 'Indeed', 'True', 'Yeah', ' Yes', 'Yep'], 'False': ['No', 'no', 'Nope', ' No', ' no', 'Wrong', 'NO', 'False']}\n"
     ]
    }
   ],
   "source": [
    "with open(DATA / \"answer_map.json\", \"r\") as f:\n",
    "    answer_map = json.load(f)\n",
    "\n",
    "print(answer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "331081c9201c4ee0b838de19c0ef8f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 00:41:50,042 - root - WARNING - You are not using LayerNorm, so the writing weights can't be centered! Skipping\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model google/gemma-2b-it into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model = HookedTransformer.from_pretrained(\"google/gemma-2b-it\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get model activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logit_diff import df_to_logits\n",
    "\n",
    "logits, cache = df_to_logits(model, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As said before, we want to understand how the model stores and retrieves factual knowledge. To do so, as in every MI investigation, we have to define a task that allows us to evaluate LLM's on our task, factual knowledge. There are a few things to consider when engineering this task:\n",
    "1. Most importantly: We want to isolate the concept we are testing for, here, factual knowledge, as much as possible. Good performance on our task should imply that the tasked model is good at the exact concept we are interested in. Conversly, if a model does not perform good on our task, this should imply this model is not good at this concept.\n",
    "2. Our generation should only be one token long for easy evaluation. It would be nice if one could determine another token that would be expected if the model were not able to correctly answer the query. These two tokens can be used for introducing logit difference as evaluation metric to the task.\n",
    "3. It should be easy to scale the task to many different examples to rule out randomness.\n",
    "4. It should be possible, only by changing a few tokens, to remove the concept from the task and make it random. This alternative distributions is needed for path patching and ablation.\n",
    "\n",
    "For this time-constrained assessment, I will have to focus on a specific example. I chose to inspect factual knowledge around the French city \"Paris\". Paris is well-known enough to be in pretty much every pre-training dataset, yet yields enough diversity to ask mutiple questions in order to prove generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under consideration of all these requirements, I selected a question-answer format for factual knowledge as task. This allows to isolate specific factual knowledge, yields one-token generations, scales pefectly, and makes it possible to \"disarm\" the query by removing the \"key\".\n",
    "\n",
    "An example, which I will use a lot throughout this assessment, is: <br>\n",
    "    - Query: Is Paris the capital of France? <br>\n",
    "    - Label: True <br>\n",
    "    - Opposite Generation: False <br>\n",
    "    - Clue: capital, France <br>\n",
    "\n",
    "I admit that this task has the drawback that I am quite sure that question-answering is a unique circuit worth studying by itself. I expect the question-answering circuits and the factual knowledge circuit to have an overlap which means I did not 100% isolate our concept. Nonetheless, after a lot of back-and-forth, I believe this is the best way of testing factual knowledge for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the model is able to confidently solve this task in the first place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Is Paris the capital of France?\\n\"\n",
    "ground_truth = \"Yes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Note</summary>\n",
    "It seems like I have to add the '\\n' tokens at the end of the query for this model to answer properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Is', ' Paris', ' the', ' capital', ' of', ' France', '?', '\\n']\n",
      "Tokenized answer: ['Yes']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">        Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32.09</span><span style=\"font-weight: bold\"> Prob: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.34</span><span style=\"font-weight: bold\">% Token: |Yes|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m        Logit: \u001b[0m\u001b[1;36m32.09\u001b[0m\u001b[1m Prob: \u001b[0m\u001b[1;36m35.34\u001b[0m\u001b[1m% Token: |Yes|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.54 Prob: 55.63% Token: |Sure|\n",
      "Top 1th token. Logit: 32.09 Prob: 35.34% Token: |Yes|\n",
      "Top 2th token. Logit: 29.96 Prob:  4.22% Token: |No|\n",
      "Top 3th token. Logit: 29.90 Prob:  3.96% Token: |Paris|\n",
      "Top 4th token. Logit: 26.54 Prob:  0.14% Token: |Answer|\n",
      "Top 5th token. Logit: 26.24 Prob:  0.10% Token: |Certainly|\n",
      "Top 6th token. Logit: 26.02 Prob:  0.08% Token: |The|\n",
      "Top 7th token. Logit: 25.92 Prob:  0.07% Token: |Of|\n",
      "Top 8th token. Logit: 25.76 Prob:  0.06% Token: |Correct|\n",
      "Top 9th token. Logit: 25.70 Prob:  0.06% Token: |*|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'Yes'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'Yes'\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "utils.test_prompt(query, ground_truth, model, prepend_bos=True, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks good, we see the first \"inconvenience\" in our task here. The model is more than capable of answering the query. Yet, it is not able to concentrate all its probability mass on one \"True-token\" such as \"Yes\" but spreads it among multiple such as \"Sure\" (56% probability), \"Yes\" (35% probability), and \"Certainly\" (0.1% probability). This is most likely because assertion of something being true is most definitely done by using all these words in the training data. While this does not make the generation wrong, it raises two questions:\n",
    "1. Is the model abel to understand that all these tokens express \"True\" and build a circuit out of that?\n",
    "2. How do we evaluate the task if we cannot measure it's outcome based on the logit/probability of a single token?\n",
    "\n",
    "For the first question, I do not have a concrete answer but, for the sake of this experiment, I will just hope that the model chosen here is indeed able to abstract the answers into \"Fact is True\" and \"Fact is False\". Larger models such as ChatGPT 4o are definitely able to do so. This model achieved close to 100% accuracy on 10 simple factual knowledge questions around Paris under the condition of answering with True or False only. For the second question, we will have to find a solution to capture all possible True- and False-tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through a bunch of example questions, I have mapped all True- and Wrong-Tokens to their corresponding underlying meaning. Although, computationally, it is not very elegant (especially bc lists have diff lenghts), we will have to always evalaute the task on all these tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'True': ['Yes',\n",
       "  'yes',\n",
       "  'Sure',\n",
       "  'Correct',\n",
       "  'Certainly',\n",
       "  'Absolutely',\n",
       "  'Indeed',\n",
       "  'True',\n",
       "  'Yeah',\n",
       "  ' Yes',\n",
       "  'Yep'],\n",
       " 'False': ['No', 'no', 'Nope', ' No', ' no', 'Wrong', 'NO', 'False']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import answer_map_to_tokens\n",
    "\n",
    "answer_map_tokens = answer_map_to_tokens(model, answer_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'question': [query], 'answer': [\"True\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logit difference of True and False tokens summed up is 113.51 nats\n"
     ]
    }
   ],
   "source": [
    "from src.logit_diff import logits_to_logit_diff\n",
    "\n",
    "logits, cache = df_to_logits(model, df)\n",
    "logit_diff = logits_to_logit_diff(df, logits, answer_map_tokens, device)\n",
    "print(f\"Logit difference of True and False tokens summed up is {logit_diff:.2f} nats\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When accounting for all True and False token, the model is clearly able to classifiy the statemnt as True. If the model were able to concentrate the probability mass of all True-Tokens on one token and that of all Wrong-Tokens on another token, the True-Token would have an $e^{113}\\approx 10^{49}\\times$ higher probability.\n",
    "\n",
    "<details> <summary>Note</summary>\n",
    "Considering typcial logit distributions over the vocabulary at the end of a Transformer it is questionable if the model, even if it were possible to concentrate on two differnet tokens, would assign such a high logit to the True token. Furhtermore, we have to consider that way more positive tokens are under the top 10 tokens which is why the differnece is so big in the first place. Nevertheless, this supports our hypothesis that the model can retrieve factual knowledge and use it to answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get some more example testing factual knowledge around Paris to make sure our results are representative. I will use OpenAI's ChatGPT-4o for dataset generation and subsequently filter to only include valid examples. This might seem unnecessary for our experiment since there are not too many qeustion-answer pairs around Paris such a small model can answer but I this makes it easy to adapt this script to other use-cases in the future and most of the code is simply recycled..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-2024-08-06\"\n",
    "query = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"Please generate 10 questions about Paris that can be answered with \"\n",
    "            \"either Yes or No. Annotate each question with True if the answer is \"\n",
    "            \"Yes and False if the answer is No. The questions should be easy to \"\n",
    "            \"answer for any human. Return the output as a JSON object with each \"\n",
    "            \"entry having the keys 'question' and 'answer' filled with a list of \"\n",
    "            \"10 values. There should be 5 questions with answer Yes and 5 \"\n",
    "            \"questions with answer no. Each question should end with a question \"\n",
    "            \"mark followed by the newline character.\\n\"\n",
    "            \"Example question: Is Paris the capital of France?\\n\"\n",
    "            \"Example answer: True\"\n",
    "        )\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ClientResponse(https://api.openai.com/v1/chat/completions) [200 OK]>\n",
      "<CIMultiDictProxy('Date': 'Tue, 27 Aug 2024 06:46:09 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Access-Control-Expose-Headers': 'X-Request-ID', 'openai-organization': 'christho', 'openai-processing-ms': '1597', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=15552000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '500', 'x-ratelimit-limit-tokens': '30000', 'x-ratelimit-remaining-requests': '499', 'x-ratelimit-remaining-tokens': '29839', 'x-ratelimit-reset-requests': '120ms', 'x-ratelimit-reset-tokens': '322ms', 'x-request-id': 'req_93641be8a2979f84886fd20018b82b7d', 'CF-Cache-Status': 'DYNAMIC', 'Set-Cookie': '__cf_bm=E6mjzpJjB4phGe_rDZ250lAe1Z2Usi5zfzSvmGlXXlY-1724741169-1.0.1.1-nGS5FQU7uw2OAtNyv9CdaXp4ZlTHYb9T9dzqs.k00_sXPB3xMbb2XXBO6LuP.bhLXebW.gD4kn4soWpG7a3Jhw; path=/; expires=Tue, 27-Aug-24 07:16:09 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'X-Content-Type-Options': 'nosniff', 'Set-Cookie': '_cfuvid=9uLcahpQ3bs_1afFFb6A7Pwbm6jXn_tEFA7MelgpEcE-1724741169764-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None', 'Server': 'cloudflare', 'CF-RAY': '8b9a1a4b8d8b10a1-ORD', 'Content-Encoding': 'br', 'alt-svc': 'h3=\":443\"; ma=86400')>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "from src.data import invoke_openai\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "dataset = asyncio.run(invoke_openai(model_name, query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset[0])\n",
    "df = pd.concat([df, dataset], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_c5168 th {\n",
       "  text-align: left;\n",
       "}\n",
       "#T_c5168_row0_col0, #T_c5168_row0_col1, #T_c5168_row1_col0, #T_c5168_row1_col1, #T_c5168_row2_col0, #T_c5168_row2_col1, #T_c5168_row3_col0, #T_c5168_row3_col1, #T_c5168_row4_col0, #T_c5168_row4_col1, #T_c5168_row5_col0, #T_c5168_row5_col1, #T_c5168_row6_col0, #T_c5168_row6_col1, #T_c5168_row7_col0, #T_c5168_row7_col1, #T_c5168_row8_col0, #T_c5168_row8_col1, #T_c5168_row9_col0, #T_c5168_row9_col1 {\n",
       "  text-align: left;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_c5168\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_c5168_level0_col0\" class=\"col_heading level0 col0\" >question</th>\n",
       "      <th id=\"T_c5168_level0_col1\" class=\"col_heading level0 col1\" >answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_c5168_row0_col0\" class=\"data row0 col0\" >Is Paris the capital of France?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row0_col1\" class=\"data row0 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_c5168_row1_col0\" class=\"data row1 col0\" >Is Paris the capital of France?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row1_col1\" class=\"data row1 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_c5168_row2_col0\" class=\"data row2 col0\" >Is the Eiffel Tower located in Paris?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row2_col1\" class=\"data row2 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_c5168_row3_col0\" class=\"data row3 col0\" >Is Paris known as the City of Light?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row3_col1\" class=\"data row3 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_c5168_row4_col0\" class=\"data row4 col0\" >Is the Louvre Museum in Rome?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row4_col1\" class=\"data row4 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_c5168_row5_col0\" class=\"data row5 col0\" >Is the official language of Paris English?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row5_col1\" class=\"data row5 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_c5168_row6_col0\" class=\"data row6 col0\" >Is Montmartre a district in Paris?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row6_col1\" class=\"data row6 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_c5168_row7_col0\" class=\"data row7 col0\" >Does Paris have a desert climate?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row7_col1\" class=\"data row7 col1\" >False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_c5168_row8_col0\" class=\"data row8 col0\" >Is the Seine River in Paris?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row8_col1\" class=\"data row8 col1\" >True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_c5168_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_c5168_row9_col0\" class=\"data row9 col0\" >Is Paris larger in area than New York City?\n",
       "</td>\n",
       "      <td id=\"T_c5168_row9_col1\" class=\"data row9 col1\" >False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f6d8477a2c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10).style.set_properties(**{'text-align': 'left'}).set_table_styles([dict(selector='th', props=[('text-align', 'left')])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's make sure our model is able to answer all questions correctly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = df_to_logits(model, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per prompt logit difference: tensor([ 103.9010,  103.9010,   89.8990,  146.5240,  -50.1270,  -63.0340,\n",
      "         125.9540,  -24.3910,  108.8900, -118.3550,   91.9470])\n",
      "Average logit difference: tensor(46.8300)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Per prompt logit difference:\",\n",
    "    logits_to_logit_diff(df, logits, answer_map_tokens, device, per_prompt=True)\n",
    "    .cpu()\n",
    "    .round(decimals=3),\n",
    ")\n",
    "print(\n",
    "    \"Average logit difference:\",\n",
    "    logits_to_logit_diff(df, logits, answer_map_tokens, device)\n",
    "    .clone()\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .round(decimals=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm for three different samples we get a negative logit difference. Let's have a look into these cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Is', ' the', ' Louvre', ' Museum', ' in', ' Rome', '?', '\\n']\n",
      "Tokenized answer: ['False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23.06</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m19\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m23.06\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;3;91mFalse\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 34.64 Prob: 93.46% Token: |No|\n",
      "Top 1th token. Logit: 31.77 Prob:  5.32% Token: |The|\n",
      "Top 2th token. Logit: 29.80 Prob:  0.74% Token: |no|\n",
      "Top 3th token. Logit: 28.31 Prob:  0.17% Token: |Yes|\n",
      "Top 4th token. Logit: 28.00 Prob:  0.12% Token: | no|\n",
      "Top 5th token. Logit: 27.82 Prob:  0.10% Token: |Sure|\n",
      "Top 6th token. Logit: 26.24 Prob:  0.02% Token: |While|\n",
      "Top 7th token. Logit: 26.20 Prob:  0.02% Token: | No|\n",
      "Top 8th token. Logit: 25.54 Prob:  0.01% Token: |NO|\n",
      "Top 9th token. Logit: 24.60 Prob:  0.00% Token: |Actually|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'False'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'False'\u001b[0m, \u001b[1;36m19\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Is', ' the', ' official', ' language', ' of', ' Paris', ' English', '?', '\\n']\n",
      "Tokenized answer: ['False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.32</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m97\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m19.32\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;3;91mFalse\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 32.93 Prob: 95.15% Token: |No|\n",
      "Top 1th token. Logit: 29.18 Prob:  2.22% Token: |The|\n",
      "Top 2th token. Logit: 28.97 Prob:  1.81% Token: |Paris|\n",
      "Top 3th token. Logit: 27.31 Prob:  0.34% Token: |Yes|\n",
      "Top 4th token. Logit: 26.17 Prob:  0.11% Token: |no|\n",
      "Top 5th token. Logit: 25.53 Prob:  0.06% Token: |Official|\n",
      "Top 6th token. Logit: 25.33 Prob:  0.05% Token: |Sure|\n",
      "Top 7th token. Logit: 24.85 Prob:  0.03% Token: |There|\n",
      "Top 8th token. Logit: 24.57 Prob:  0.02% Token: | No|\n",
      "Top 9th token. Logit: 24.41 Prob:  0.02% Token: |Is|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'False'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">97</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'False'\u001b[0m, \u001b[1;36m97\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Does', ' Paris', ' have', ' a', ' desert', ' climate', '?', '\\n']\n",
      "Tokenized answer: ['False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">       Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">25.34</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m14\u001b[0m\u001b[1m       Logit: \u001b[0m\u001b[1;36m25.34\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;3;91mFalse\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 35.83 Prob: 90.46% Token: |No|\n",
      "Top 1th token. Logit: 33.55 Prob:  9.26% Token: |Paris|\n",
      "Top 2th token. Logit: 29.38 Prob:  0.14% Token: |no|\n",
      "Top 3th token. Logit: 27.71 Prob:  0.03% Token: |The|\n",
      "Top 4th token. Logit: 27.09 Prob:  0.01% Token: | No|\n",
      "Top 5th token. Logit: 26.98 Prob:  0.01% Token: |Sure|\n",
      "Top 6th token. Logit: 26.93 Prob:  0.01% Token: |Yes|\n",
      "Top 7th token. Logit: 26.91 Prob:  0.01% Token: | Paris|\n",
      "Top 8th token. Logit: 26.69 Prob:  0.01% Token: |It|\n",
      "Top 9th token. Logit: 26.24 Prob:  0.01% Token: |While|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'False'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">14</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'False'\u001b[0m, \u001b[1;36m14\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Is', ' Paris', ' larger', ' in', ' area', ' than', ' New', ' York', ' City', '?', '\\n']\n",
      "Tokenized answer: ['False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">      Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.16</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m257\u001b[0m\u001b[1m      Logit: \u001b[0m\u001b[1;36m19.16\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;3;91mFalse\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 35.04 Prob: 56.73% Token: |Paris|\n",
      "Top 1th token. Logit: 34.51 Prob: 33.41% Token: |Sure|\n",
      "Top 2th token. Logit: 33.06 Prob:  7.84% Token: |Yes|\n",
      "Top 3th token. Logit: 30.97 Prob:  0.97% Token: |No|\n",
      "Top 4th token. Logit: 30.33 Prob:  0.51% Token: |The|\n",
      "Top 5th token. Logit: 29.99 Prob:  0.37% Token: | Paris|\n",
      "Top 6th token. Logit: 27.90 Prob:  0.05% Token: |While|\n",
      "Top 7th token. Logit: 27.80 Prob:  0.04% Token: |Certainly|\n",
      "Top 8th token. Logit: 26.12 Prob:  0.01% Token: |New|\n",
      "Top 9th token. Logit: 25.90 Prob:  0.01% Token: |That|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'False'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'False'\u001b[0m, \u001b[1;36m257\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [4, 5, 7, 9]:\n",
    "    utils.test_prompt(df[\"question\"].iloc[i], df[\"answer\"].iloc[i], model, prepend_bos=True, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a clear drawback of our method. As anticipated in the notes earlier one, having more True-Tokens than False-Tokens in our evaluation methods, dilutes the quality of the metric as the logits of tokens seem to have a lower bound which is why a lot of meaningless logits (Close to 0% token probability have almost as high logits as more probable tokens). Let's add a `return_probs` flag to our `logits_to_logits_diff` function to sum the probability of the tokens. Considering the property that all token probabilities add to one, this is more insightful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.logit_diff import logits_to_logit_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per prompt logit difference: tensor([ 0.6320,  0.6320,  0.3670,  0.9000,  0.8930,  0.8810,  0.9740,  0.9430,\n",
      "         0.7010, -0.4030,  0.5180])\n",
      "Average logit difference: tensor(0.6400)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Per prompt logit difference:\",\n",
    "    logits_to_logit_diff(df, logits, answer_map_tokens, device, return_probs=True, per_prompt=True)\n",
    "    .cpu()\n",
    "    .round(decimals=3),\n",
    ")\n",
    "print(\n",
    "    \"Average logit difference:\",\n",
    "    logits_to_logit_diff(df, logits, answer_map_tokens, device, return_probs=True)\n",
    "    .clone()\n",
    "    .detach()\n",
    "    .cpu()\n",
    "    .round(decimals=2),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'True': ['Yes',\n",
       "  'yes',\n",
       "  'Sure',\n",
       "  'Correct',\n",
       "  'Certainly',\n",
       "  'Absolutely',\n",
       "  'Indeed',\n",
       "  'True',\n",
       "  'Yeah',\n",
       "  ' Yes',\n",
       "  'Yep'],\n",
       " 'False': ['No', 'no', 'Nope', ' No', ' no', 'Wrong', 'NO', 'False']}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well well, see how quick things turn. Let's look at the samples with $p_{diff} < 0.15$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized prompt: ['<bos>', 'Is', ' Paris', ' larger', ' in', ' area', ' than', ' New', ' York', ' City', '?', '\\n']\n",
      "Tokenized answer: ['False']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n",
       "<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">      Logit: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.16</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00</span><span style=\"font-weight: bold\">% Token: |</span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold; font-style: italic\">False</span><span style=\"font-weight: bold\">|</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Performance on answer token:\n",
       "\u001b[1mRank: \u001b[0m\u001b[1;36m257\u001b[0m\u001b[1m      Logit: \u001b[0m\u001b[1;36m19.16\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.00\u001b[0m\u001b[1m% Token: |\u001b[0m\u001b[1;3;91mFalse\u001b[0m\u001b[1m|\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 0th token. Logit: 35.04 Prob: 56.73% Token: |Paris|\n",
      "Top 1th token. Logit: 34.51 Prob: 33.41% Token: |Sure|\n",
      "Top 2th token. Logit: 33.06 Prob:  7.84% Token: |Yes|\n",
      "Top 3th token. Logit: 30.97 Prob:  0.97% Token: |No|\n",
      "Top 4th token. Logit: 30.33 Prob:  0.51% Token: |The|\n",
      "Top 5th token. Logit: 29.99 Prob:  0.37% Token: | Paris|\n",
      "Top 6th token. Logit: 27.90 Prob:  0.05% Token: |While|\n",
      "Top 7th token. Logit: 27.80 Prob:  0.04% Token: |Certainly|\n",
      "Top 8th token. Logit: 26.12 Prob:  0.01% Token: |New|\n",
      "Top 9th token. Logit: 25.90 Prob:  0.01% Token: |That|\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">'False'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">257</span><span style=\"font-weight: bold\">)]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'False'\u001b[0m, \u001b[1;36m257\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [9]:\n",
    "    utils.test_prompt(df[\"question\"].iloc[i], df[\"answer\"].iloc[i], model, prepend_bos=True, prepend_space_to_answer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks way better. Now we have an insightful metric to evaluate our task. Let's remove the sample with index 9, save the dataframe, and move on to the fun part..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_idx = [9]\n",
    "df.drop(drop_idx, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataset:\n",
      "                                       question answer\n",
      "0             Is Paris the capital of France?\\n   True\n",
      "1             Is Paris the capital of France?\\n   True\n",
      "2       Is the Eiffel Tower located in Paris?\\n   True\n",
      "3        Is Paris known as the City of Light?\\n   True\n",
      "4               Is the Louvre Museum in Rome?\\n  False\n",
      "5  Is the official language of Paris English?\\n  False\n",
      "6          Is Montmartre a district in Paris?\\n   True\n",
      "7           Does Paris have a desert climate?\\n  False\n",
      "8                Is the Seine River in Paris?\\n   True\n",
      "9        Is the Mona Lisa displayed in Paris?\\n   True\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final dataset:\\n{df}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DATA / 'fk_samples.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset (Shortcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA / 'fk_samples.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Paris the capital of France?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is Paris the capital of France?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is the Eiffel Tower located in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Paris known as the City of Light?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the Louvre Museum in Rome?\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Is the official language of Paris English?\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is Montmartre a district in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Does Paris have a desert climate?\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Is the Seine River in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is the Mona Lisa displayed in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  answer\n",
       "0             Is Paris the capital of France?\\n    True\n",
       "1             Is Paris the capital of France?\\n    True\n",
       "2       Is the Eiffel Tower located in Paris?\\n    True\n",
       "3        Is Paris known as the City of Light?\\n    True\n",
       "4               Is the Louvre Museum in Rome?\\n   False\n",
       "5  Is the official language of Paris English?\\n   False\n",
       "6          Is Montmartre a district in Paris?\\n    True\n",
       "7           Does Paris have a desert climate?\\n   False\n",
       "8                Is the Seine River in Paris?\\n    True\n",
       "9        Is the Mona Lisa displayed in Paris?\\n    True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct Logit Attribution (Logit Lens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of direct logit attribution is to quantify how the different components in our transformer affect the outcome of our task. To do so, we use the residual stream direction of our logit difference and observe how the dot product between this vector and the embedding of the final token in the residual stream behaves, layer by layer. Since unembedding and layernorm, the two transformations applied to the final state of the residual stream are approxamately linear, we can expedite the evaluation step to the final residual stream embeddings. Unfortunately, softmax, the final transformation applied to get to token probabilities, the evaluation metric we deemed most useful for our use-case, is not linear. Hence, we cannot directly translate the a positive/negative change in direct logit attribution into an equal change in our previously selected metric, the difference in accumulated token probability. Yet, the sign and the relative magnitude of the change in direct logit attribution can be interpreted for our evaluation metric, allowing us to discover layer more relevant for our task than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create an answer map, mapping the the True-/False-Tokens to their corresponding embeddings (we already did this for the last step too)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import answer_map_to_tokens\n",
    "\n",
    "answer_map_tokens = answer_map_to_tokens(model, answer_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get the residual stream directiosn of our True-/False-Tokens. This means nothing else than that we retrieve the column-vectors in the unembedding matrix that will dictate the logit of the tokens we are interested in via dot product with the input vector. We then sum those unembedding vector for the True- and False-Tokens respectively and substract the True-Token embeddings from the False-Token sum of unembeddings (or vice versa) according to our label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Paris the capital of France?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is Paris the capital of France?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is the Eiffel Tower located in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Is Paris known as the City of Light?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is the Louvre Museum in Rome?\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Is the official language of Paris English?\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Is Montmartre a district in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Does Paris have a desert climate?\\n</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Is the Seine River in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Is the Mona Lisa displayed in Paris?\\n</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       question  answer\n",
       "0             Is Paris the capital of France?\\n    True\n",
       "1             Is Paris the capital of France?\\n    True\n",
       "2       Is the Eiffel Tower located in Paris?\\n    True\n",
       "3        Is Paris known as the City of Light?\\n    True\n",
       "4               Is the Louvre Museum in Rome?\\n   False\n",
       "5  Is the official language of Paris English?\\n   False\n",
       "6          Is Montmartre a district in Paris?\\n    True\n",
       "7           Does Paris have a desert climate?\\n   False\n",
       "8                Is the Seine River in Paris?\\n    True\n",
       "9        Is the Mona Lisa displayed in Paris?\\n    True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.direct_logit_attribution import get_answer_residual_direction\n",
    "\n",
    "logit_diff_directions = get_answer_residual_direction(\n",
    "    model,\n",
    "    df,\n",
    "    answer_map_tokens,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we use the embedding of the residual stream for the last token in our sequence at every point in our network where the output of a layer is fed back into the residual stream to plot the value of the answer residual direction after each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "hovertemplate": "<b>%{hovertext}</b><br><br>x=%{x}<br>y=%{y}<extra></extra>",
         "hovertext": [
          "0_pre",
          "0_mid",
          "1_pre",
          "1_mid",
          "2_pre",
          "2_mid",
          "3_pre",
          "3_mid",
          "4_pre",
          "4_mid",
          "5_pre",
          "5_mid",
          "6_pre",
          "6_mid",
          "7_pre",
          "7_mid",
          "8_pre",
          "8_mid",
          "9_pre",
          "9_mid",
          "10_pre",
          "10_mid",
          "11_pre",
          "11_mid",
          "12_pre",
          "12_mid",
          "13_pre",
          "13_mid",
          "14_pre",
          "14_mid",
          "15_pre",
          "15_mid",
          "16_pre",
          "16_mid",
          "17_pre",
          "17_mid",
          "final_post"
         ],
         "legendgroup": "",
         "line": {
          "color": "#636efa",
          "dash": "solid"
         },
         "marker": {
          "symbol": "circle"
         },
         "mode": "lines",
         "name": "",
         "orientation": "v",
         "showlegend": false,
         "type": "scatter",
         "x": [
          0,
          0.5,
          1,
          1.5,
          2,
          2.5,
          3,
          3.5,
          4,
          4.5,
          5,
          5.5,
          6,
          6.5,
          7,
          7.5,
          8,
          8.5,
          9,
          9.5,
          10,
          10.5,
          11,
          11.5,
          12,
          12.5,
          13,
          13.5,
          14,
          14.5,
          15,
          15.5,
          16,
          16.5,
          17,
          17.5,
          18
         ],
         "xaxis": "x",
         "y": [
          -151.4582977294922,
          -58.21533966064453,
          -18.473119735717773,
          -9.18809700012207,
          -1.3914753198623657,
          0.13373719155788422,
          3.0478150844573975,
          4.867020606994629,
          5.349269866943359,
          4.8563055992126465,
          5.16217565536499,
          6.5401153564453125,
          9.183932304382324,
          8.068795204162598,
          8.455453872680664,
          8.692374229431152,
          9.944613456726074,
          10.655824661254883,
          13.407376289367676,
          14.483624458312988,
          14.884442329406738,
          16.294910430908203,
          18.19736099243164,
          18.69903564453125,
          25.190603256225586,
          24.577051162719727,
          28.730802536010742,
          29.059171676635742,
          56.71091842651367,
          54.39046096801758,
          71.39568328857422,
          71.39311981201172,
          77.5388412475586,
          76.34412384033203,
          75.15583801269531,
          83.00133514404297,
          66.64376831054688
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Logit Difference From Accumulate Residual Stream"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "x"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "y"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"569e57bc-3252-4020-bcf6-098ff38940fd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"569e57bc-3252-4020-bcf6-098ff38940fd\")) {                    Plotly.newPlot(                        \"569e57bc-3252-4020-bcf6-098ff38940fd\",                        [{\"hovertemplate\":\"\\u003cb\\u003e%{hovertext}\\u003c\\u002fb\\u003e\\u003cbr\\u003e\\u003cbr\\u003ex=%{x}\\u003cbr\\u003ey=%{y}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"hovertext\":[\"0_pre\",\"0_mid\",\"1_pre\",\"1_mid\",\"2_pre\",\"2_mid\",\"3_pre\",\"3_mid\",\"4_pre\",\"4_mid\",\"5_pre\",\"5_mid\",\"6_pre\",\"6_mid\",\"7_pre\",\"7_mid\",\"8_pre\",\"8_mid\",\"9_pre\",\"9_mid\",\"10_pre\",\"10_mid\",\"11_pre\",\"11_mid\",\"12_pre\",\"12_mid\",\"13_pre\",\"13_mid\",\"14_pre\",\"14_mid\",\"15_pre\",\"15_mid\",\"16_pre\",\"16_mid\",\"17_pre\",\"17_mid\",\"final_post\"],\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.0,0.5,1.0,1.5,2.0,2.5,3.0,3.5,4.0,4.5,5.0,5.5,6.0,6.5,7.0,7.5,8.0,8.5,9.0,9.5,10.0,10.5,11.0,11.5,12.0,12.5,13.0,13.5,14.0,14.5,15.0,15.5,16.0,16.5,17.0,17.5,18.0],\"xaxis\":\"x\",\"y\":[-151.4582977294922,-58.21533966064453,-18.473119735717773,-9.18809700012207,-1.3914753198623657,0.13373719155788422,3.0478150844573975,4.867020606994629,5.349269866943359,4.8563055992126465,5.16217565536499,6.5401153564453125,9.183932304382324,8.068795204162598,8.455453872680664,8.692374229431152,9.944613456726074,10.655824661254883,13.407376289367676,14.483624458312988,14.884442329406738,16.294910430908203,18.19736099243164,18.69903564453125,25.190603256225586,24.577051162719727,28.730802536010742,29.059171676635742,56.71091842651367,54.39046096801758,71.39568328857422,71.39311981201172,77.5388412475586,76.34412384033203,75.15583801269531,83.00133514404297,66.64376831054688],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"x\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"y\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Logit Difference From Accumulate Residual Stream\"}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('569e57bc-3252-4020-bcf6-098ff38940fd');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.direct_logit_attribution import residual_stack_to_logit_diff\n",
    "from src.utils import line\n",
    "\n",
    "accumulated_residual, labels = cache.accumulated_resid(\n",
    "    layer=-1, incl_mid=True, pos_slice=-1, return_labels=True\n",
    ")\n",
    "logit_lens_logit_diffs = residual_stack_to_logit_diff(\n",
    "    accumulated_residual, \n",
    "    logit_diff_directions, \n",
    "    cache\n",
    ")\n",
    "line(\n",
    "    logit_lens_logit_diffs,\n",
    "    x=np.arange(model.cfg.n_layers * 2 + 1) / 2,\n",
    "    hover_name=labels,\n",
    "    title=\"Logit Difference From Accumulate Residual Stream\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that even with this slightly flawed metric, the model is continuously improving. The plot is almost monotonic. It starts with the embedding of the last tokens which yield a highly negative logit difference. This makes sense since the last token (next line token) is not our answer. With almost every layer, the logit difference increases. We have a high slope in the beginning (layer 1), followed by a rather steady increase (layer 2-13.5), a jump in capability at MLP of layer 13 (~40% of overall increase between 0 and final logit difference), and finally another steady increase up to the last MLP at which we see a little drop again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summed increase in self-attention layer after layer 1: 20.38\n",
      "Summed increase in MLP layer after layer 1: 64.74\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.direct_logit_attribution import increase_per_layer_type\n",
    "\n",
    "increase_per_layer_type(logit_lens_logit_diffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we neglect the first layer, we observe that the logit difference increases much more in MLP layer than self-attention layer. Over the 17 remaining layer, the logit difference in the residual stream increases by 20.38 nat over self-attention layer and 64.74 nat (70% of total increase) over MLP layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FK-in-MI Environment",
   "language": "python",
   "name": "fk-in-mi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
